{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a6a083",
   "metadata": {},
   "source": [
    "# Tracking your model\n",
    "\n",
    "First things first, we will now separate the training of the model, in a file called `train.py` from the serving of the model, in a file called `serve.py`. That is a good first step. But that won't be enough.\n",
    "\n",
    "Because, what are we going to do once that model has been trained ?\n",
    "\n",
    "## Saving your models\n",
    "\n",
    "Our first model was the simplest model you could ever imagine : simply answering the same label every time. It takes no time to train. That's why we did not even bother to save it.\n",
    "\n",
    "If your model takes minutes, hours or even days to train, you can't train it at the startup of your container. You are going to train it and save it. Then at the startup of your container, you will load it. Loading is supposedly faster than training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef4f509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DummyClassifier(constant=0, strategy=&#x27;constant&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyClassifier</label><div class=\"sk-toggleable__content\"><pre>DummyClassifier(constant=0, strategy=&#x27;constant&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DummyClassifier(constant=0, strategy='constant')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should be in a file called train.py\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "iris_dataset =  load_iris()\n",
    "X,  y = iris_dataset[\"data\"], iris_dataset[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n",
    "dummy_classifier = DummyClassifier(strategy=\"constant\", constant=y[0])\n",
    "dummy_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98a968",
   "metadata": {},
   "source": [
    "Now we want to save it. We are going to use the built-in solution for Python persistence : `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d543082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"./dumb_model.pkl\"\n",
    "\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(dummy_classifier , f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428d473",
   "metadata": {},
   "source": [
    "Are we sure this is the same model ? It might be altered from the persistence process. Let's check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1252de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "    \n",
    "assert (loaded_model.predict(X_test[0:10]) == dummy_classifier.predict(X_test[0:10])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f1f15",
   "metadata": {},
   "source": [
    "Yay ! It seems to produce the same results as the one before pickling.\n",
    "So now we can put this code into the `serve.py` file, that looks almost the same as the `main.py`.\n",
    "\n",
    "This step is very important : we have introduced a decoupling between the training and the serving. I personally think that this is the most important thing to do, and that everything is straightforward from there.\n",
    "\n",
    "## Organizing your models\n",
    "\n",
    "Before we dive any further, let's think about everything we will need for the next step.\n",
    "\n",
    "* Models need to be stored and easily retrievable for your teammates, and for your business applications, cloud apps included\n",
    "* Models should be unpickled in an environment similar to the one they have been pickled in\n",
    "* When you create a new model, you should be able to compare it to the one in production\n",
    "* It should be easy to know **which model is in production**\n",
    "\n",
    "Basically, what you could do on your own, is storing your models in a cloud bucket, such as [Amazon S3](https://aws.amazon.com/en/s3/), [Google Cloud Storage](https://cloud.google.com/storage/docs/creating-buckets), [Azure Blob Storage](https://azure.microsoft.com/fr-fr/products/storage/blobs/) or whatever.\n",
    "\n",
    "You could also log all the metada related to the model, the environment from which it has been pickled, and the results of your experiments, so you can compare its performance later with other models.\n",
    "\n",
    "That's a lot of dev. Good news : it's called an ML Model Registry, and somebody has already done it for you. Today we are going to use [MLFlow](https://mlflow.org/), but there are some others ([neptune.ai](https://neptune.ai/) or [Amazon SageMaker Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html) for example).\n",
    "\n",
    "Of course you should use it with an external storage, like a cloud storage, but we are just toying around, it will be easier to store the models locally (plus, this way we don't have to pay for a cloud storage service).\n",
    "\n",
    "## Organizing your services\n",
    "\n",
    "Ok now things will get a little bit more _DevOpsy_ and a little lest _data sciencish_. We will need at least two containers :\n",
    "- one that runs the MLFlow server\n",
    "- one that runs a PostgreSQL database\n",
    "\n",
    "Actually, there should be a third container to run your training, but I'll save you this step for this time.\n",
    "The MLFlow container should be accessible from your notebooks, and the MLFlow server should be able to communicate with the Postgres container.\n",
    "\n",
    "To do all this locally, you can use Docker Compose. So ... bye bye Python, hello YAML. This what your` docker-compose.yaml` :\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "  db:\n",
    "    image: \"postgres\"\n",
    "    environment:\n",
    "      POSTGRES_USER: mlflow\n",
    "      POSTGRES_PASSWORD: mlflow\n",
    "\n",
    "  mlflow:\n",
    "    build: ./mlflow\n",
    "    ports:\n",
    "      - \"5001:5000\"\n",
    "    expose:\n",
    "      - 5000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc36dd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'sk-learn-dummy-model'.\n",
      "2022/10/10 13:32:20 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: sk-learn-dummy-model, version 1\n",
      "Created version '1' of model 'sk-learn-dummy-model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"EXAMPLE_RUN\") as run:\n",
    "    # Log the sklearn model and register as version 1\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=dummy_classifier,\n",
    "        artifact_path=\"sklearn-model\",\n",
    "        registered_model_name=\"sk-learn-dummy-model\"\n",
    "    )\n",
    "\n",
    "client = MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"sk-learn-dummy-model\",\n",
    "    version=1,\n",
    "    stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54637a10",
   "metadata": {},
   "source": [
    "Wow ! We now have logged our model in the Mlflow Model Registry, and we tagged it for Production from the training side. It's now time to fetch this model from the registry on the serving side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d95dc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "model_name = \"sk-learn-dummy-model\"\n",
    "model_version = 1\n",
    "\n",
    "fetched_model = mlflow.pyfunc.load_model(\n",
    "    model_uri=f\"models:/{model_name}/{model_version}\"\n",
    ")\n",
    "\n",
    "assert (fetched_model.predict(X_test[0:10]) == dummy_classifier.predict(X_test[0:10])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca6bd2",
   "metadata": {},
   "source": [
    "Remember, we need to do this in a `serve.py` file. This begs the question : when should we fetch the model. The simplest way (and this is what we are going to do at the step 2 of our journey), is to fetch it at the startup of our webservice. But there are other ways : embedding it in the container at build time, load it from a mounted volume etc, or even use dedicated tools that do all the heavy lifting for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406febf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
