{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba98249",
   "metadata": {},
   "source": [
    "# Creating a model\n",
    "\n",
    "## Getting a dataset\n",
    "\n",
    "For now, this will be a very simple step. We will be using a scikit-learn dataset, the now famous Iris dataset. It is small, easily downloadable on your laptop, and already cleaned up. Later, we will deal with dirty datasets, too big too load on a single machine.\n",
    "\n",
    "But for now, we want to focus on the architecture, and the fastest way to shipping a model to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aff05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset =  load_iris()\n",
    "X,  y = iris_dataset[\"data\"], iris_dataset[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843fead",
   "metadata": {},
   "source": [
    "## Getting a training set and a test set\n",
    "\n",
    "The purpose of this course is not to focus on ML best practices, but you have to understand at least the basics of it. Dividing your dataset in train, test and validation will have a great influence on the final architecture of your MLOps processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b08e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193b879",
   "metadata": {},
   "source": [
    "## Training a baseline model\n",
    "\n",
    "We are using a dummy classifier, because the most important thing when thinking MLOps, is not spending a lot of time having the perfect model. The best model is not the one running in your Jupyter Notebook, but the one in production, adding value to your application. So any baseline that is good enough should be shipped to production ASAP.\n",
    "\n",
    "Since the purpose of these examples is not focusing on ML itself, but MLOps, we will push even further this logic by using the dumbest model possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_classifier = DummyClassifier(strategy=\"constant\", constant=y[0])\n",
    "dummy_classifier.fit(X_train,y_train)\n",
    "\n",
    "accuracy = dummy_classifier.score(X_test, y_test)\n",
    "f\"Accuracy : {100*accuracy:.1f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b8d82",
   "metadata": {},
   "source": [
    "See how dumb that model is ? Accuracy is only 39.5%. I'm not even pretending to care. We are going to ship this one, then we will continuously improve this model with the CI until it's (almost) state-of-the-art.\n",
    "\n",
    "\"Regular\" code is built by a CI. Nowadays, nobody (I hope), ships code that has been build locally on his machine. Well it should be the same for model training in machine learning. Your model should be trained by a CI (or something equivalent, like Airflow).\n",
    "\n",
    "For now, the most important thing is shipping this model to prod. Even if it's so bad it could hurt your business. Because even if it's in production, nobody will be actually listening to your endpoint. But it's there, and it's actually the most difficult thing to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e97a24",
   "metadata": {},
   "source": [
    "# Serving a model\n",
    "\n",
    "There are various ways to serve a model, ie making it available for some external service to call it, and have the model return a prediction or a label.\n",
    "\n",
    "The most ubiquitous and simple way is just encapsulating it in a webservice. Soooo ... that's what we are going to do now !\n",
    "\n",
    "## Serving a model (locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "class Iris(BaseModel):\n",
    "    sepal_length: float\n",
    "    sepal_width: float\n",
    "    petal_length: float\n",
    "    petal_width: float\n",
    "        \n",
    "        \n",
    "class Label(BaseModel):\n",
    "    label: int\n",
    "            \n",
    "    \n",
    "@app.post(\"/label\", response_model=Label)\n",
    "async def label(iris: Iris):\n",
    "    iris_data = [[iris.sepal_length, iris.sepal_width, iris.petal_length, iris.petal_width]]\n",
    "    label = dummy_classifier.predict(iris_data)\n",
    "    return Label(label=label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb18c2d",
   "metadata": {},
   "source": [
    "You could just run this code snippet, go to http://127.0.0.1:8000/docs and toy around with the API (and see how cool FastAPI is), but if you want to have a quick feedback loop, it is best to have small unit tests.\n",
    "\n",
    "Here is one for example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d28d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.testclient import TestClient\n",
    "\n",
    "\n",
    "def test_is_response_has_correct_form():\n",
    "    client = TestClient(app)\n",
    "    response = client.post(\"/label\",\n",
    "                           json={\"sepal_length\": 0,\n",
    "                                 \"sepal_width\": 0,\n",
    "                                 \"petal_length\": 0,\n",
    "                                 \"petal_width\": 0}).json()\n",
    "    assert \"label\" in response\n",
    "    assert response[\"label\"] in [0, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f1b1b",
   "metadata": {},
   "source": [
    "We'll see later how we can leverage the different kinds of tests and how we can leverage them to improve our MLOps feedback loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dff98",
   "metadata": {},
   "source": [
    "## Serving a model (for real)\n",
    "\n",
    "If you're a data scientist reading this, serving your model might be a huge step outside of your regular attributions.\n",
    "\n",
    "But this is not how grown-ups serve their models. Time to take an other extra step. Yes, we are going to bundle it in a Docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e5a67",
   "metadata": {},
   "source": [
    "```Dockerfile\n",
    "# python base image\n",
    "FROM python:3.10 \n",
    "\n",
    "#expose the port 8000 (could be another, but this is the default port for FastAPI)\n",
    "EXPOSE 8000\n",
    "\n",
    "# this is the working directory\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "RUN pip install pipenv\n",
    "\n",
    "COPY ./Pipfile* .\n",
    "# this is necessary when pipenv run inside a container\n",
    "RUN pipenv install --deploy\n",
    "\n",
    "COPY ./main.py .\n",
    "\n",
    "# the command that will be launched in the container\n",
    "CMD [\"pipenv\", \"run\",\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ce42c",
   "metadata": {},
   "source": [
    "This is not the place (nor I feel qualified) to explain how Docker and the Dockerfile work. But I added some comments line by line.\n",
    "\n",
    "Now you need to :\n",
    "1. build the image\n",
    "2. launch a container with that image\n",
    "\n",
    "For this, I created for you a little ... Makefile ! Way easier to use than script files. Want to build ? Just type `make build` in your favourite command line.\n",
    "\n",
    "Want to run the container ? Type `make run`, go to http://localhost:8000/docs, and see how the magic happens.\n",
    "\n",
    "Want to kill the container ? Type `make kill`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362c921",
   "metadata": {},
   "source": [
    "```make\n",
    "build:\n",
    "\tdocker build . -t basic-model-serve\n",
    "\n",
    "run:\n",
    "\tdocker run --name basic-model-serve --rm -d -p 8000:8000 basic-model-serve\n",
    "\n",
    "kill:\n",
    "\tdocker kill basic-model-serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95455c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this first example, we have seen how to build a baseline model and encapsulate it in a webservice, and bundle the whole thing in a Docker container. This is a standard practice these days, because of the omnipresence of the cloud.\n",
    "\n",
    "Of course, there are other ways to use your models, another one would be to make your predictions in batch. Maybe you don't need a real time prediction, maybe you can just label all your dataset during the night, to make it available during the day for your users.\n",
    "\n",
    "There are a lot of things that could be done in a better way. For example, you should not be training your model at the startup of your webservice. This thing takes time, and your container orchestration service will probably think that your container is not working correctly, and kill it right away. But we will improve every part of this chain in the next chapters.\n",
    "\n",
    "For now, we have to improve that pretty dumb model of ours, and making the CI do all of the hard work for us."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
